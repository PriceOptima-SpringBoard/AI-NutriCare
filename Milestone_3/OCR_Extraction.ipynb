{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7aef50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\ainutricare\\nutricare\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\ainutricare\\nutricare\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\ainutricare\\nutricare\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\ainutricare\\nutricare\\lib\\site-packages (0.11.9)\n",
      "Requirement already satisfied: pandas in c:\\ainutricare\\nutricare\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pdfminer.six==20251230 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pdfplumber) (20251230)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pdfplumber) (5.3.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\ainutricare\\nutricare\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\ainutricare\\nutricare\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract pdf2image pillow pdfplumber pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2d2353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting text from: C:\\AINutriCare\\Data\\Raw\\Reports\\REPORT.pdf\n",
      "\n",
      "=== Extracted Parameters ===\n",
      "Glucose     : 157.07 mg/dL\n",
      "Insulin     : None ¬µIU/mL\n",
      "Creatinine  : 0.83 mg/dL\n",
      "Urea (BUN)  : None mg/dL\n",
      "Sodium      : 143.00 mmol/L\n",
      "Potassium   : 4.90 mmol/L\n",
      "Hemoglobin  : 14.5 g/dL\n",
      "WBC         : 10570 /cmm\n",
      "Lactate     : None mmol/L\n",
      "pH          : 6.0 \n",
      "Age         : None years\n",
      "Gender      : Tube \n",
      "Cholestrol  : 189.0 mg/dL\n",
      "HbA1c       : 7.10 %\n",
      "[INFO] Saved vitals JSON to: patient_vitals.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "\n",
    "# ---- YOUR PDF PATH ----\n",
    "PDF_PATH = r\"C:\\AINutriCare\\Data\\Raw\\Reports\\REPORT.pdf\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "PARAMETERS = [\n",
    "    \"Glucose\",\n",
    "    \"Insulin\",\n",
    "    \"Creatinine\",\n",
    "    \"Urea (BUN)\",\n",
    "    \"Sodium\",\n",
    "    \"Potassium\",\n",
    "    \"Hemoglobin\",\n",
    "    \"WBC\",\n",
    "    \"Lactate\",\n",
    "    \"pH\",\n",
    "    \"Age\",\n",
    "    \"Gender\",\n",
    "    \"Cholestrol\",\n",
    "    \"HbA1c\",\n",
    "]\n",
    "\n",
    "\n",
    "# Updated patterns to match investigationlabreports.pdf layout[file:45]\n",
    "# Updated patterns to handle OCR typos (e.g., 'l' vs '1', 'g/dl' vs 'g/d1')\n",
    "PATTERN_MAP = {\n",
    "    # Glucose: Matches \"78 mg/dl\"\n",
    "    \"Glucose\": r\"GLUCOSE[^\\n]*?([0-9.]+)\\s*mg/[dl1I]+\",\n",
    "\n",
    "    # Insulin: Not present in this report (Expected: None)\n",
    "    \"Insulin\": r\"(Insulin)\\s+([0-9.]+)\",\n",
    "\n",
    "    # Creatinine: Matches \"0.86 # mg/dl\"\n",
    "    \"Creatinine\": r\"CREATININE[^\\n]*?([0-9.]+)\\s*#?\\s*mg/[dl1I]+\",\n",
    "\n",
    "    # Urea: Matches \"9.81 mg/dl\"\n",
    "    \"Urea (BUN)\": r\"BUN[^\\n]*?([0-9.]+)\\s*mg/[dl1I]+\",\n",
    "\n",
    "    # Sodium: Matches \"139.0 mmol/l\" (Handles mmol/1, mmol/I)\n",
    "    \"Sodium\": r\"SODIUM[^\\n]*?([0-9.]+)\\s*mmol/[l1I|]+\",\n",
    "\n",
    "    # Potassium: Matches \"4.01 mmol/l\"\n",
    "    \"Potassium\": r\"POTASSIUM[^\\n]*?([0-9.]+)\\s*mmol/[l1I|]+\",\n",
    "\n",
    "    # Hemoglobin: Matches \"15.1 g/dl\" (Handles g/d1, g/dl)\n",
    "    \"Hemoglobin\": r\"[Hh]a?emoglobin[^\\n]*?([0-9.]+)\\s*g/[dl1I]+\",\n",
    "\n",
    "    # WBC: Matches \"8800 /cu.mm\" or \"8800 /cmm\"\n",
    "    \"WBC\": r\"WBC\\s+Count.*?([0-9]+)\\s*/(?:cu\\.mm|cmm)\",\n",
    "\n",
    "    # Lactate: Not present in this report\n",
    "    \"Lactate\": r\"(Lactate)\\s+([0-9.]+)\",\n",
    "    \n",
    "    # pH: Not present in this report\n",
    "    \"pH\": r\"\\bpH\\b\\s*([0-9.]+)\",\n",
    "\n",
    "    # Age: Captures number after \"Age :\"\n",
    "    \"Age\": r\"Age\\s*:\\s*([0-9]{1,3})\",\n",
    "    \n",
    "    # Gender: Captures text after \"Sex :\" or \"Gender :\"\n",
    "    \"Gender\": r\"(?:Sex|Gender)\\s*:\\s*([A-Za-z]+)\",\n",
    "\n",
    "    # Cholesterol: Not present in this report\n",
    "    \"Cholestrol\": r\"(Cholesterol)\\s+([0-9.]+)\",\n",
    "    \n",
    "    # HbA1c: Not present in this report\n",
    "    \"HbA1c\": r\"HbA1c\\s*.*?([0-9.]+)\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "UNIT_MAP = {\n",
    "    \"Glucose\": \"mg/dL\",\n",
    "    \"Insulin\": \"¬µIU/mL\",\n",
    "    \"Creatinine\": \"mg/dL\",\n",
    "    \"Urea (BUN)\": \"mg/dL\",\n",
    "    \"Sodium\": \"mmol/L\",\n",
    "    \"Potassium\": \"mmol/L\",\n",
    "    \"Hemoglobin\": \"g/dL\",\n",
    "    \"WBC\": \"/cmm\",\n",
    "    \"Lactate\": \"mmol/L\",\n",
    "    \"pH\": \"\",\n",
    "    \"Age\": \"years\",\n",
    "    \"Gender\": \"\",\n",
    "    \"Cholestrol\": \"mg/dL\",\n",
    "    \"HbA1c\": \"%\",\n",
    "}\n",
    "\n",
    "\n",
    "def ocr_pdf_to_text(pdf_path: str) -> str:\n",
    "    print(f\"[INFO] Extracting text from: {pdf_path}\")\n",
    "    text_pages: List[str] = []\n",
    "\n",
    "    # 1) Try direct text\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text_pages.append(page_text)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] pdfplumber failed: {e}\")\n",
    "\n",
    "    full_text = \"\\n\".join(text_pages).strip()\n",
    "\n",
    "    # 2) Fallback to OCR if too little text\n",
    "    if len(full_text) < 200:\n",
    "        print(\"[INFO] Direct text small; switching to OCR...\")\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        ocr_pages = []\n",
    "        for i, img in enumerate(images):\n",
    "            page_text = pytesseract.image_to_string(img)\n",
    "            ocr_pages.append(page_text)\n",
    "            print(f\"[INFO] OCR page {i+1}/{len(images)} done.\")\n",
    "        full_text = \"\\n\".join(ocr_pages)\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def extract_parameter(text: str, parameter: str) -> Dict[str, Any]:\n",
    "    pattern = PATTERN_MAP.get(parameter)\n",
    "    result = {\n",
    "        \"name\": parameter,\n",
    "        \"value\": None,\n",
    "        \"unit\": UNIT_MAP.get(parameter, \"\"),\n",
    "        \"raw_match\": \"\",\n",
    "    }\n",
    "    if not pattern:\n",
    "        return result\n",
    "\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "    if not match:\n",
    "        return result\n",
    "\n",
    "    value_str = match.groups()[-1]\n",
    "    result[\"value\"] = value_str.strip()\n",
    "    result[\"raw_match\"] = match.group(0).strip()\n",
    "    return result\n",
    "\n",
    "    # Always use the last captured group as the numeric/string value\n",
    "    value_str = match.groups()[-1]\n",
    "    result[\"value\"] = value_str.strip()\n",
    "    result[\"raw_match\"] = match.group(0).strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_all_parameters(text: str) -> Dict[str, Dict[str, Any]]:\n",
    "    data = {}\n",
    "    for param in PARAMETERS:\n",
    "        data[param] = extract_parameter(text, param)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_json(data: Dict[str, Dict[str, Any]], out_json: str):\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"[INFO] Saved vitals JSON to: {out_json}\")\n",
    "\n",
    "\n",
    "def run_extraction():\n",
    "    out_json = \"patient_vitals.json\"\n",
    "    full_text = ocr_pdf_to_text(PDF_PATH)\n",
    "\n",
    "    extracted = extract_all_parameters(full_text)\n",
    "\n",
    "    print(\"\\n=== Extracted Parameters ===\")\n",
    "    for k, v in extracted.items():\n",
    "        print(f\"{k:12s}: {v.get('value')} {v.get('unit')}\")\n",
    "\n",
    "    save_to_json(extracted, out_json)\n",
    "    return extracted\n",
    "\n",
    "\n",
    "# In Jupyter, just run this cell:\n",
    "if __name__ == \"__main__\":\n",
    "    extracted_params = run_extraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7889f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AINutriCare\\Nutricare\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI Model & Scaler...\n",
      "WARNING:tensorflow:From c:\\AINutriCare\\Nutricare\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AI Resources Loaded Successfully.\n",
      "Running LSTM Prediction...\n",
      "\n",
      "Processing AI Clinical Analysis...\n",
      "‚úÖ Success! Analysis saved to: clinical_output.json\n",
      "{\n",
      "  \"patient_metrics\": {\n",
      "    \"mortality_risk\": 0.40944451093673706,\n",
      "    \"glucose\": 157.07,\n",
      "    \"creatinine\": 0.83\n",
      "  },\n",
      "  \"conditions\": [\n",
      "    \"Moderate Clinical Risk\",\n",
      "    \"Diabetes (Type 2 / Hyperglycemia)\"\n",
      "  ],\n",
      "  \"avoid\": [\n",
      "    \"Fruit juices\",\n",
      "    \"White bread\",\n",
      "    \"Processed sugars\",\n",
      "    \"High-GI foods\"\n",
      "  ],\n",
      "  \"recommend\": [\n",
      "    \"Complex carbohydrates\",\n",
      "    \"High fiber foods (>30g/day)\",\n",
      "    \"Leafy greens\"\n",
      "  ],\n",
      "  \"summary\": \"Patient requires dietary management and monitoring.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Paths\n",
    "# ==========================================\n",
    "# Ensure these paths match your local setup\n",
    "JSON_INPUT = \"patient_vitals.json\"\n",
    "MODEL_PATH = r\"C:\\AINutriCare\\Notebooks\\Milestone_2\\LSTM\\attention_lstm.h5\"\n",
    "SCALER_PATH = r\"C:\\AINutriCare\\Data\\Transformed\\X_final.npy\"\n",
    "\n",
    "# The 17 Features the Model was trained on (Must be in this exact order)\n",
    "MODEL_FEATURES = [\n",
    "    \"Heart Rate\", \"MAP\", \"Respiratory Rate\", \"Temperature\", \n",
    "    \"Glucose\", \"Creatinine\", \"BUN\", \"Sodium\", \"Potassium\", \"Hemoglobin\", \"WBC\", \"Lactate\",\n",
    "    \"Fluid Balance\", \"Vasopressors\", \"Sedatives\", \"Antibiotics\", \"Insulin\"\n",
    "]\n",
    "\n",
    "# Defaults for values NOT in the PDF (Assumes resting/stable state for missing vitals)\n",
    "DEFAULTS = {\n",
    "    'Heart Rate': 75, 'MAP': 90, 'Respiratory Rate': 16, 'Temperature': 98.4,\n",
    "    'Lactate': 1.0, 'Fluid Balance': 0, 'Vasopressors': 0, 'Sedatives': 0, \n",
    "    'Antibiotics': 0, 'Insulin': 0\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. Define Custom Layer (Required to load model)\n",
    "# ==========================================\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class SimpleAttention(Layer):\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    def get_config(self):\n",
    "        config = super(SimpleAttention, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(name='att_w1', shape=(input_shape[-1], self.units), initializer='glorot_uniform')\n",
    "        self.W2 = self.add_weight(name='att_w2', shape=(self.units, 1), initializer='glorot_uniform')\n",
    "        self.b1 = self.add_weight(name='att_b1', shape=(self.units,), initializer='zeros')\n",
    "        super(SimpleAttention, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        h = tf.nn.tanh(tf.matmul(x, self.W1) + self.b1)\n",
    "        e = tf.squeeze(tf.matmul(h, self.W2), -1)\n",
    "        alpha = tf.nn.softmax(e)\n",
    "        context = x * tf.expand_dims(alpha, -1)\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "        return context, alpha\n",
    "\n",
    "# ==========================================\n",
    "# 3. Load Resources\n",
    "# ==========================================\n",
    "def load_ai_resources():\n",
    "    print(\"Loading AI Model & Scaler...\")\n",
    "    try:\n",
    "        # 1. Load Model\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
    "        model = load_model(MODEL_PATH, custom_objects={'SimpleAttention': SimpleAttention})\n",
    "        \n",
    "        # 2. Load Scaler Statistics (Mean/Std from Training)\n",
    "        if os.path.exists(SCALER_PATH):\n",
    "            X_ref = np.load(SCALER_PATH)\n",
    "            X_flat = X_ref.reshape(-1, X_ref.shape[2])\n",
    "            means = np.mean(X_flat, axis=0)\n",
    "            stds = np.std(X_flat, axis=0)\n",
    "            stds[stds == 0] = 1.0 # Prevent divide by zero\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Scaler file (X_final.npy) not found. Using raw unscaled values (Results may be inaccurate).\")\n",
    "            means = np.zeros(17)\n",
    "            stds = np.ones(17)\n",
    "            \n",
    "        print(\"‚úÖ AI Resources Loaded Successfully.\")\n",
    "        return model, means, stds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error Loading AI Resources: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ==========================================\n",
    "# 4. Data Processing (JSON -> Tensor)\n",
    "# ==========================================\n",
    "def preprocess_patient_data(json_file, means, stds):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # helper to safely get float values\n",
    "    def get_val(key):\n",
    "        item = data.get(key)\n",
    "        if isinstance(item, dict):\n",
    "            val = item.get('value')\n",
    "        else:\n",
    "            val = item\n",
    "            \n",
    "        if val in [None, \"N/A\", \"Not Found\"]: return None\n",
    "        try:\n",
    "            # Clean string like \"H 141.0\" -> 141.0\n",
    "            clean = str(val).replace('H', '').replace('L', '').strip()\n",
    "            return float(clean)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # 1. Build Feature Vector (17,)\n",
    "    vector = []\n",
    "    extracted_values = {} # For reporting\n",
    "    \n",
    "    for feature in MODEL_FEATURES:\n",
    "        # Map Model Feature Names to JSON Keys\n",
    "        json_key = feature\n",
    "        if feature == \"Cholesterol\": json_key = \"Cholestrol\" # Handle typo in json if present\n",
    "        \n",
    "        val = get_val(json_key)\n",
    "        \n",
    "        # Fallback Logic\n",
    "        if val is None:\n",
    "            val = DEFAULTS.get(feature, 0)\n",
    "        \n",
    "        # Scaling fixes (e.g. WBC 10570 -> 10.57)\n",
    "        if feature == \"WBC\" and val > 1000:\n",
    "            val = val / 1000.0\n",
    "            \n",
    "        vector.append(val)\n",
    "        extracted_values[feature] = val\n",
    "\n",
    "    # 2. Create Time Series (Repeat static data for 24 hours)\n",
    "    # Shape: (1, 24, 17)\n",
    "    patient_matrix = np.tile(vector, (24, 1))\n",
    "    \n",
    "    # 3. Normalize\n",
    "    normalized_matrix = (patient_matrix - means) / stds\n",
    "    input_tensor = normalized_matrix.reshape(1, 24, 17)\n",
    "    \n",
    "    return input_tensor, extracted_values, data\n",
    "\n",
    "# ==========================================\n",
    "# 5. Diet Logic (Post-Prediction)\n",
    "# ==========================================\n",
    "def generate_clinical_report(risk_score, vitals, raw_json):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" AI CLINICAL ANALYSIS REPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- 1. Risk Interpretation ---\n",
    "    if risk_score > 0.60:\n",
    "        status = \"HIGH RISK (Critical)\"\n",
    "        action = \"Immediate Metabolic Intervention\"\n",
    "    elif risk_score > 0.30:\n",
    "        status = \"MODERATE RISK\"\n",
    "        action = \"Dietary Management & Monitoring\"\n",
    "    else:\n",
    "        status = \"STABLE\"\n",
    "        action = \"Routine Maintenance\"\n",
    "        \n",
    "    print(f\"\\n[1] MODEL PREDICTION\")\n",
    "    print(f\"    Mortality/ICU Risk: {risk_score:.2%}\")\n",
    "    print(f\"    Clinical Status:    {status}\")\n",
    "    print(f\"    Recommended Action: {action}\")\n",
    "\n",
    "    # --- 2. Key Drivers ---\n",
    "    print(f\"\\n[2] BIOMARKER ANALYSIS\")\n",
    "    \n",
    "    # Check Diabetes\n",
    "    gluc = vitals['Glucose']\n",
    "    hba1c_val = raw_json.get('HbA1c', {}).get('value', 'N/A')\n",
    "    print(f\"    - Glucose: {gluc} mg/dL\", end=\"\")\n",
    "    if gluc > 140: print(\" (HIGH - Driver for Risk)\")\n",
    "    else: print(\" (Normal)\")\n",
    "    \n",
    "    print(f\"    - HbA1c:   {hba1c_val} %\", end=\"\")\n",
    "    try:\n",
    "        if float(str(hba1c_val).replace('H','')) > 6.5: print(\" (DIABETIC RANGE)\")\n",
    "        else: print(\"\")\n",
    "    except: print(\"\")\n",
    "\n",
    "    # Check Renal\n",
    "    creat = vitals['Creatinine']\n",
    "    print(f\"    - Creatinine: {creat} mg/dL\", end=\"\")\n",
    "    if creat > 1.2: print(\" (RENAL STRESS)\")\n",
    "    else: print(\" (Normal)\")\n",
    "\n",
    "    # --- 3. Diet Plan ---\n",
    "    print(f\"\\n[3] AI-GENERATED NUTRITION PLAN\")\n",
    "    \n",
    "    if gluc > 126 or (hba1c_val != 'N/A' and float(str(hba1c_val).replace('H','')) > 6.5):\n",
    "        print(\"    Protocol: DIABETIC / LOW-GLYCEMIC INDEX\")\n",
    "        print(\"    - Carbohydrates: Restricted to 40% of total calories.\")\n",
    "        print(\"    - Focus: Complex carbs only (Fiber > 30g/day).\")\n",
    "        print(\"    - Avoid: Fruit juices, white bread, processed sugars.\")\n",
    "    elif risk_score > 0.5:\n",
    "        print(\"    Protocol: CRITICAL CARE SUPPORT (High Protein)\")\n",
    "        print(\"    - Focus: Preventing muscle wasting (Catabolism).\")\n",
    "    else:\n",
    "        print(\"    Protocol: STANDARD BALANCED DIET\")\n",
    "        print(\"    - Maintain current nutritional intake.\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "def generate_clinical_report(risk_score, vitals, raw_json):\n",
    "    \"\"\"\n",
    "    Analyzes prediction & vitals to create a JSON for the LLM.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing AI Clinical Analysis...\")\n",
    "\n",
    "    # --- 1. Initialize Structure for LLM ---\n",
    "    llm_context = {\n",
    "        \"patient_metrics\": {\n",
    "            \"mortality_risk\": float(risk_score),  # JSON needs native float, not numpy\n",
    "            \"glucose\": float(vitals['Glucose']),\n",
    "            \"creatinine\": float(vitals['Creatinine'])\n",
    "        },\n",
    "        \"conditions\": [],\n",
    "        \"avoid\": [],\n",
    "        \"recommend\": [],\n",
    "        \"summary\": \"\"\n",
    "    }\n",
    "\n",
    "    # --- 2. Risk Interpretation ---\n",
    "    if risk_score > 0.60:\n",
    "        llm_context['conditions'].append(\"Critical Stability Risk\")\n",
    "        llm_context['summary'] = \"Patient is at HIGH RISK. Immediate metabolic intervention required.\"\n",
    "    elif risk_score > 0.30:\n",
    "        llm_context['conditions'].append(\"Moderate Clinical Risk\")\n",
    "        llm_context['summary'] = \"Patient requires dietary management and monitoring.\"\n",
    "    else:\n",
    "        llm_context['summary'] = \"Patient is stable. Routine maintenance diet recommended.\"\n",
    "\n",
    "    # --- 3. Biomarker Analysis (Logic -> Rules) ---\n",
    "    \n",
    "    # Check Diabetes / Glucose\n",
    "    glucose = vitals['Glucose']\n",
    "    hba1c_val = raw_json.get('HbA1c', {}).get('value', 'N/A')\n",
    "    \n",
    "    is_diabetic = False\n",
    "    if glucose > 126:\n",
    "        is_diabetic = True\n",
    "    # Handle H141 type strings if present in raw json\n",
    "    if hba1c_val != 'N/A':\n",
    "        try:\n",
    "            val = float(str(hba1c_val).replace('H','').replace('L',''))\n",
    "            if val > 6.5: is_diabetic = True\n",
    "        except: pass\n",
    "\n",
    "    if is_diabetic:\n",
    "        llm_context['conditions'].append(\"Diabetes (Type 2 / Hyperglycemia)\")\n",
    "        llm_context['avoid'].extend([\"Fruit juices\", \"White bread\", \"Processed sugars\", \"High-GI foods\"])\n",
    "        llm_context['recommend'].extend([\"Complex carbohydrates\", \"High fiber foods (>30g/day)\", \"Leafy greens\"])\n",
    "\n",
    "    # Check Renal (Kidneys)\n",
    "    creatinine = vitals['Creatinine']\n",
    "    if creatinine > 1.2:\n",
    "        llm_context['conditions'].append(\"Renal Stress / Kidney Strain\")\n",
    "        llm_context['avoid'].extend([\"High sodium foods\", \"Excessive red meat\", \"Processed deli meats\"])\n",
    "        llm_context['recommend'].extend([\"Low-potassium vegetables\", \"Cauliflower\", \"Berries\"])\n",
    "\n",
    "    # Check Hypertension (using MAP as proxy if BP not split)\n",
    "    # MAP > 100 often correlates with high BP\n",
    "    if vitals['MAP'] > 100: \n",
    "        llm_context['conditions'].append(\"Hypertension Risk\")\n",
    "        llm_context['avoid'].append(\"Salt/Sodium\")\n",
    "        llm_context['recommend'].append(\"DASH diet principles\")\n",
    "\n",
    "    # If no specific conditions found, add general healthy advice\n",
    "    if not llm_context['conditions']:\n",
    "        llm_context['conditions'].append(\"General Health Maintenance\")\n",
    "        llm_context['recommend'].append(\"Balanced diet with lean proteins and vegetables\")\n",
    "\n",
    "    return llm_context\n",
    "\n",
    "# ==========================================\n",
    "# 6. Main Execution Loop\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Model\n",
    "    model, means, stds = load_ai_resources()\n",
    "    \n",
    "    if model:\n",
    "        # 2. Process Data\n",
    "        if os.path.exists(JSON_INPUT):\n",
    "            input_tensor, vitals_dict, raw_data = preprocess_patient_data(JSON_INPUT, means, stds)\n",
    "            \n",
    "            # 3. Predict\n",
    "            print(\"Running LSTM Prediction...\")\n",
    "            prediction = model.predict(input_tensor, verbose=0)[0][0]\n",
    "            \n",
    "            # 4. Generate & Save Report\n",
    "            ai_output = generate_clinical_report(prediction, vitals_dict, raw_data)\n",
    "            \n",
    "            # Output Filename\n",
    "            OUTPUT_FILE = \"clinical_output.json\"\n",
    "            \n",
    "            with open(OUTPUT_FILE, 'w') as f:\n",
    "                json.dump(ai_output, f, indent=4)\n",
    "                \n",
    "            print(f\"‚úÖ Success! Analysis saved to: {OUTPUT_FILE}\")\n",
    "            print(json.dumps(ai_output, indent=2)) # Print preview\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ùå Error: {JSON_INPUT} not found. Run the extraction step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f8446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.58.0-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-genai) (4.12.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.47.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-genai) (2.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sniffio (from google-genai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\ainutricare\\nutricare\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\ainutricare\\nutricare\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\ainutricare\\nutricare\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\ainutricare\\nutricare\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\ainutricare\\nutricare\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\ainutricare\\nutricare\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\ainutricare\\nutricare\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\ainutricare\\nutricare\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\ainutricare\\nutricare\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.58.0-py3-none-any.whl (718 kB)\n",
      "   ---------------------------------------- 0.0/718.4 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 524.3/718.4 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 718.4/718.4 kB 3.0 MB/s  0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: websockets, tenacity, sniffio, distro, google-genai\n",
      "\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   ---------------------------------------- 0/5 [websockets]\n",
      "   -------- ------------------------------- 1/5 [tenacity]\n",
      "   -------- ------------------------------- 1/5 [tenacity]\n",
      "   -------- ------------------------------- 1/5 [tenacity]\n",
      "   ---------------- ----------------------- 2/5 [sniffio]\n",
      "   ------------------------ --------------- 3/5 [distro]\n",
      "   ------------------------ --------------- 3/5 [distro]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   -------------------------------- ------- 4/5 [google-genai]\n",
      "   ---------------------------------------- 5/5 [google-genai]\n",
      "\n",
      "Successfully installed distro-1.9.0 google-genai-1.58.0 sniffio-1.3.1 tenacity-9.1.2 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770378e2",
   "metadata": {},
   "source": [
    "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\n",
    "AIzaSyD_lIMzf-pDJhSOZoEo8o5PtZUik6Yit6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d26a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "TC-01: PASS - Glucose reduces, risk lowered\n",
      "TC-02: PASS - Creatinine stabilizes\n",
      "TC-03: PASS - Sodium decreases\n",
      "TC-04: PASS - Overall risk reduced\n",
      "TC-05: PASS - BP improves\n",
      "TC-06: PASS - Hydration maintained\n",
      "TC-07: PASS - Stable health\n",
      "TC-08: PASS - Safe diet generated\n",
      "TC-09: PASS - Consistent logic\n",
      "TC-10: PASS - Pipeline stable\n",
      "\n",
      "Saved per-test-case diet plans to diet_plans_by_testcase.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "TEST_CSV = \"Diet_AI_Model_Test_Cases.csv\"\n",
    "AGG_OUTPUT = \"diet_plans_by_testcase.json\"\n",
    "\n",
    "def run_tests_and_collect():\n",
    "    df_cases = pd.read_csv(TEST_CSV)\n",
    "    food_df = load_and_tag_data(FOOD_KB_FILE)\n",
    "\n",
    "    results = []\n",
    "    all_plans = {}  # <- aggregated per-test-case\n",
    "\n",
    "    for _, row in df_cases.iterrows():\n",
    "        tc_id = row[\"Test Case ID\"]\n",
    "\n",
    "        clinical_insights = build_clinical_from_row(row)\n",
    "        candidates = get_smart_candidates(food_df, clinical_insights)\n",
    "        patient = {\"name\": tc_id, \"age\": 50}\n",
    "\n",
    "        plan = generate_structured_plan(patient, clinical_insights, candidates)\n",
    "        if \"error\" in plan:\n",
    "            results.append((tc_id, \"FAIL\", f\"Generator error: {plan['error']}\"))\n",
    "            continue\n",
    "\n",
    "        # store only the foods (you can store full plan if you prefer)\n",
    "        all_plans[tc_id] = {\n",
    "            \"clinical_scenario\": row[\"Clinical Scenario\"],\n",
    "            \"input_labs\": row[\"Input Labs\"],\n",
    "            \"doctor_notes\": row[\"Doctor Notes\"],\n",
    "            \"must_not_include\": row[\"Must NOT Include\"],\n",
    "            \"day_plan\": plan.get(\"day_plan\", {}),\n",
    "            \"total_nutrition\": plan.get(\"total_nutrition\", {}),\n",
    "        }\n",
    "\n",
    "        ok, reason = check_forbidden(plan, row[\"Must NOT Include\"])\n",
    "        if not ok:\n",
    "            results.append((tc_id, \"FAIL\", reason))\n",
    "        else:\n",
    "            results.append((tc_id, \"PASS\", row[\"Expected Outcome\"]))\n",
    "\n",
    "    # print summary\n",
    "    for tc_id, status, msg in results:\n",
    "        print(f\"{tc_id}: {status} - {msg}\")\n",
    "\n",
    "    # save aggregated JSON\n",
    "    with open(AGG_OUTPUT, \"w\") as f:\n",
    "        json.dump(all_plans, f, indent=2)\n",
    "    print(f\"\\nSaved per-test-case diet plans to {AGG_OUTPUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests_and_collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f2081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Food DB...\n",
      "ü§ñ AI is structuring the JSON plan...\n",
      "\n",
      "‚úÖ GENERATED JSON:\n",
      "{\n",
      "  \"day_plan\": {\n",
      "    \"breakfast\": [\n",
      "      {\n",
      "        \"item\": \"Khandvi\",\n",
      "        \"calories\": 55.0,\n",
      "        \"protein\": 13.0,\n",
      "        \"fat\": 17.0,\n",
      "        \"carbs\": 30.0,\n",
      "        \"tags\": [\n",
      "          \"diabetic_friendly\",\n",
      "          \"low_sugar\",\n",
      "          \"high_protein\",\n",
      "          \"renal_safe\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"lunch\": [\n",
      "      {\n",
      "        \"item\": \"Galho\",\n",
      "        \"calories\": 155.0,\n",
      "        \"protein\": 13.0,\n",
      "        \"fat\": 11.0,\n",
      "        \"carbs\": 22.0,\n",
      "        \"tags\": [\n",
      "          \"diabetic_friendly\",\n",
      "          \"low_sugar\",\n",
      "          \"high_protein\",\n",
      "          \"renal_safe\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"dinner\": [\n",
      "      {\n",
      "        \"item\": \"Kolim Jawla\",\n",
      "        \"calories\": 215.0,\n",
      "        \"protein\": 11.0,\n",
      "        \"fat\": 29.0,\n",
      "        \"carbs\": 12.0,\n",
      "        \"tags\": [\n",
      "          \"diabetic_friendly\",\n",
      "          \"low_sugar\",\n",
      "          \"high_protein\",\n",
      "          \"renal_safe\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"snacks\": [\n",
      "      {\n",
      "        \"item\": \"Tandoori Fish Tikka\",\n",
      "        \"calories\": 43.0,\n",
      "        \"protein\": 8.0,\n",
      "        \"fat\": 2.0,\n",
      "        \"carbs\": 18.0,\n",
      "        \"tags\": [\n",
      "          \"diabetic_friendly\",\n",
      "          \"low_sugar\",\n",
      "          \"low_fat\",\n",
      "          \"renal_safe\"\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"total_nutrition\": {\n",
      "    \"calories\": 468.0,\n",
      "    \"protein\": 45.0,\n",
      "    \"fat\": 60.0,\n",
      "    \"carbs\": 82.0\n",
      "  },\n",
      "  \"medical_reasoning\": \"The meal plan for Rajesh Kumar focuses on items tagged as diabetic-friendly, low-sugar, high-protein, and renal-safe to support general dietary management and monitoring. This selection aims to provide balanced nutrition while adhering to common considerations for patients requiring careful dietary oversight.\"\n",
      "}\n",
      "\n",
      "Saved to 'final_structured_diet.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (GOOGLE GEMINI)\n",
    "# ==========================================\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyD_lIMzf-pDJhSOZoEo8o5PtZUik6Yit6c\"  # or set env var GEMINI_API_KEY\n",
    "MODEL_ID = \"gemini-2.5-flash\"  # choose your Gemini model\n",
    "\n",
    "# File Paths\n",
    "CLINICAL_INPUT = \"clinical_output.json\"\n",
    "FOOD_KB_FILE = \"diet_kb.json\"\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. AUTO-TAGGING & FILTERING SYSTEM\n",
    "# ==========================================\n",
    "def load_and_tag_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads diet_kb.json and adds medical tags based on macros.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return pd.DataFrame()  # Return empty if missing\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # --- AUTO-TAGGING LOGIC ---\n",
    "    def get_tags(row):\n",
    "        tags = []\n",
    "        # Diabetic Friendly: Low Carb (<30g) OR Low Sugar (ingredients check)\n",
    "        if row[\"Carbohydrate (g)\"] < 30 and \"sugar\" not in str(row[\"ingredients\"]).lower():\n",
    "            tags.append(\"diabetic_friendly\")\n",
    "            tags.append(\"low_sugar\")\n",
    "\n",
    "        # High Protein: > 10g\n",
    "        if row[\"Protein (g)\"] > 10:\n",
    "            tags.append(\"high_protein\")\n",
    "\n",
    "        # Low Fat: < 8g\n",
    "        if row[\"Total Fat (g)\"] < 8:\n",
    "            tags.append(\"low_fat\")\n",
    "\n",
    "        # Renal Safe (Simplified): Moderate Protein (5‚Äì15g)\n",
    "        if 5 < row[\"Protein (g)\"] < 15:\n",
    "            tags.append(\"renal_safe\")\n",
    "\n",
    "        return tags\n",
    "\n",
    "    df[\"medical_tags\"] = df.apply(get_tags, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_smart_candidates(df, clinical_insights):\n",
    "    \"\"\"\n",
    "    Filters the tagged dataframe to find the best candidates for the patient.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    # 1. Parse Constraints\n",
    "    conditions = \" \".join(clinical_insights.get(\"conditions\", [])).lower()\n",
    "    avoid = \" \".join(clinical_insights.get(\"avoid\", [])).lower()\n",
    "\n",
    "    # 2. Apply Filters\n",
    "    candidates = df.copy()\n",
    "\n",
    "    # DIABETES FILTER\n",
    "    if \"diabetes\" in conditions or \"sugar\" in avoid:\n",
    "        candidates = candidates[candidates[\"medical_tags\"].apply(lambda x: \"diabetic_friendly\" in x)]\n",
    "\n",
    "    # RENAL FILTER\n",
    "    if \"renal\" in conditions or \"kidney\" in conditions:\n",
    "        candidates = candidates[candidates[\"medical_tags\"].apply(lambda x: \"renal_safe\" in x)]\n",
    "\n",
    "    # 3. Categorize\n",
    "    breakfast_keywords = \"idli|dosa|upma|poha|paratha|oats|porridge\"\n",
    "    breakfast_df = candidates[candidates[\"name\"].str.contains(breakfast_keywords, case=False, na=False)]\n",
    "    if len(breakfast_df) < 2:\n",
    "        breakfast_df = candidates.sample(n=min(5, len(candidates)))\n",
    "\n",
    "    # Lunch/Dinner: high calorie mains\n",
    "    mains_df = candidates[candidates[\"Energy (kcal)\"] > 150]\n",
    "\n",
    "    # Snacks: low calorie\n",
    "    snacks_df = candidates[candidates[\"Energy (kcal)\"] < 150]\n",
    "\n",
    "    def serialize(sub_df, count=5):\n",
    "        return sub_df.sample(n=min(count, len(sub_df))).to_dict(orient=\"records\")\n",
    "\n",
    "    return {\n",
    "        \"breakfast\": serialize(breakfast_df),\n",
    "        \"lunch\": serialize(mains_df),\n",
    "        \"dinner\": serialize(mains_df),\n",
    "        \"snacks\": serialize(snacks_df),\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. LLM GENERATOR (Structure Enforcer ‚Äì GEMINI)\n",
    "# ==========================================\n",
    "def generate_structured_plan(patient_profile, clinical_data, food_candidates):\n",
    "    summary = clinical_data.get(\"summary\", \"Healthy Diet\")\n",
    "    options_preview = json.dumps(food_candidates, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI Clinical Dietitian.\n",
    "\n",
    "    PATIENT: {patient_profile['name']}\n",
    "    CONDITION: {summary}\n",
    "\n",
    "    TASK:\n",
    "    Select items from the PROVIDED CANDIDATE LIST below to create a 1-day meal plan.\n",
    "    You must output the result in STRICT JSON format matching the user's required schema.\n",
    "\n",
    "    CANDIDATE FOODS (Pick from these):\n",
    "    {options_preview}\n",
    "\n",
    "    REQUIRED OUTPUT FORMAT (JSON):\n",
    "    {{\n",
    "      \"day_plan\": {{\n",
    "          \"breakfast\": [\n",
    "              {{ \"item\": \"Name\", \"calories\": 100, \"protein\": 5, \"fat\": 2, \"carbs\": 10, \"tags\": [\"tag1\", \"tag2\"] }}\n",
    "          ],\n",
    "          \"lunch\": [],\n",
    "          \"dinner\": [],\n",
    "          \"snacks\": []\n",
    "      }},\n",
    "      \"total_nutrition\": {{\n",
    "          \"calories\": 0,\n",
    "          \"protein\": 0,\n",
    "          \"carbs\": 0,\n",
    "          \"fat\": 0\n",
    "      }},\n",
    "      \"medical_reasoning\": \"Brief explanation...\"\n",
    "    }}\n",
    "\n",
    "    RULES:\n",
    "    1. Use the EXACT nutritional values from the candidate list. Do not invent numbers.\n",
    "    2. Include the 'medical_tags' provided in the candidate list.\n",
    "    3. Calculate the 'total_nutrition' sum correctly.\n",
    "    4. Output JSON ONLY. No text before or after.\n",
    "    \"\"\"\n",
    "\n",
    "    # JSON schema ‚Äì fully specified for Gemini\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"day_plan\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"breakfast\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"item\",\n",
    "                                \"calories\",\n",
    "                                \"protein\",\n",
    "                                \"fat\",\n",
    "                                \"carbs\",\n",
    "                                \"tags\",\n",
    "                            ],\n",
    "                        },\n",
    "                    },\n",
    "                    \"lunch\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"item\",\n",
    "                                \"calories\",\n",
    "                                \"protein\",\n",
    "                                \"fat\",\n",
    "                                \"carbs\",\n",
    "                                \"tags\",\n",
    "                            ],\n",
    "                        },\n",
    "                    },\n",
    "                    \"dinner\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"item\",\n",
    "                                \"calories\",\n",
    "                                \"protein\",\n",
    "                                \"fat\",\n",
    "                                \"carbs\",\n",
    "                                \"tags\",\n",
    "                            ],\n",
    "                        },\n",
    "                    },\n",
    "                    \"snacks\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\n",
    "                                \"item\",\n",
    "                                \"calories\",\n",
    "                                \"protein\",\n",
    "                                \"fat\",\n",
    "                                \"carbs\",\n",
    "                                \"tags\",\n",
    "                            ],\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"breakfast\", \"lunch\", \"dinner\", \"snacks\"],\n",
    "            },\n",
    "            \"total_nutrition\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"calories\": {\"type\": \"number\"},\n",
    "                    \"protein\": {\"type\": \"number\"},\n",
    "                    \"fat\": {\"type\": \"number\"},\n",
    "                    \"carbs\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"calories\", \"protein\", \"fat\", \"carbs\"],\n",
    "            },\n",
    "            \"medical_reasoning\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"day_plan\", \"total_nutrition\", \"medical_reasoning\"],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"ü§ñ AI is structuring the JSON plan...\")\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=schema,\n",
    "                temperature=0.1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # In JSON mode, resp.parsed is already a Python dict\n",
    "        if hasattr(resp, \"parsed\") and resp.parsed is not None:\n",
    "            return resp.parsed\n",
    "\n",
    "        raw_text = resp.text.strip()\n",
    "        return json.loads(raw_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        raw_text = \"\"\n",
    "        try:\n",
    "            raw_text = resp.text  # may exist if request reached the model\n",
    "        except:\n",
    "            pass\n",
    "        return {\"error\": str(e), \"raw_output\": raw_text}\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. ROUNDING HELPER\n",
    "# ==========================================\n",
    "def round_plan(plan, ndigits=0):\n",
    "    for meal in [\"breakfast\", \"lunch\", \"dinner\", \"snacks\"]:\n",
    "        for dish in plan.get(\"day_plan\", {}).get(meal, []):\n",
    "            for key in [\"calories\", \"protein\", \"fat\", \"carbs\"]:\n",
    "                val = dish.get(key)\n",
    "                if isinstance(val, (int, float)):\n",
    "                    dish[key] = round(val, ndigits)\n",
    "\n",
    "    tn = plan.get(\"total_nutrition\", {})\n",
    "    for key in [\"calories\", \"protein\", \"fat\", \"carbs\"]:\n",
    "        val = tn.get(key)\n",
    "        if isinstance(val, (int, float)):\n",
    "            tn[key] = round(val, ndigits)\n",
    "    return plan\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Loading Food DB...\")\n",
    "    df = load_and_tag_data(FOOD_KB_FILE)\n",
    "\n",
    "    # Clinical insights\n",
    "    if os.path.exists(CLINICAL_INPUT):\n",
    "        with open(CLINICAL_INPUT, \"r\") as f:\n",
    "            clinical_insights = json.load(f)\n",
    "    else:\n",
    "        clinical_insights = {\n",
    "            \"conditions\": [\"Diabetes\"],\n",
    "            \"summary\": \"Patient requires low-sugar, low-carb diet.\",\n",
    "        }\n",
    "\n",
    "    candidates = get_smart_candidates(df, clinical_insights)\n",
    "\n",
    "    patient = {\"name\": \"Rajesh Kumar\", \"age\": 45}\n",
    "    final_json = generate_structured_plan(patient, clinical_insights, candidates)\n",
    "    final_json = round_plan(final_json, ndigits=0)\n",
    "\n",
    "    if \"error\" not in final_json:\n",
    "        print(\"\\n‚úÖ GENERATED JSON:\")\n",
    "        print(json.dumps(final_json, indent=2))\n",
    "\n",
    "        with open(\"final_structured_diet.json\", \"w\") as f:\n",
    "            json.dump(final_json, f, indent=2)\n",
    "            print(\"\\nSaved to 'final_structured_diet.json'\")\n",
    "    else:\n",
    "        print(\"‚ùå Error:\", final_json[\"error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4548840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0. CONFIG\n",
    "# =========================\n",
    "# Paths (update to match your setup)\n",
    "MODEL_PATH = r\"C:\\AINutriCare\\Notebooks\\Milestone_2\\LSTM\\attention_lstm.h5\"\n",
    "SCALER_PATH = r\"C:\\AINutriCare\\Data\\Transformed\\X_final.npy\"\n",
    "FOOD_KB_FILE = \"diet_kb.json\"\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"AIzaSyD_lIMzf-pDJhSOZoEo8o5PtZUik6Yit6c\")\n",
    "GEMINI_MODEL_ID = \"gemini-2.5-flash\"\n",
    "\n",
    "# Tesseract path\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ========= FastAPI app =========\n",
    "app = FastAPI(title=\"AI-NutriCare API\", version=\"0.1.0\")\n",
    "\n",
    "# CORS for React dev (adjust origin as needed)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:5173\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. OCR + PARAMETER EXTRACTION\n",
    "# =========================\n",
    "PARAMETERS = [\n",
    "    \"Glucose\", \"Insulin\", \"Creatinine\", \"Urea (BUN)\",\n",
    "    \"Sodium\", \"Potassium\", \"Hemoglobin\", \"WBC\",\n",
    "    \"Lactate\", \"pH\", \"Age\", \"Gender\", \"Cholestrol\", \"HbA1c\",\n",
    "]\n",
    "\n",
    "PATTERN_MAP = {\n",
    "    \"Glucose\": r\"GLUCOSE[^\\n]*?([0-9.]+)\\s*mg/[dl1I]+\",\n",
    "    \"Insulin\": r\"(Insulin)\\s+([0-9.]+)\",\n",
    "    \"Creatinine\": r\"CREATININE[^\\n]*?([0-9.]+)\\s*#?\\s*mg/[dl1I]+\",\n",
    "    \"Urea (BUN)\": r\"BUN[^\\n]*?([0-9.]+)\\s*mg/[dl1I]+\",\n",
    "    \"Sodium\": r\"SODIUM[^\\n]*?([0-9.]+)\\s*mmol/[l1I|]+\",\n",
    "    \"Potassium\": r\"POTASSIUM[^\\n]*?([0-9.]+)\\s*mmol/[l1I|]+\",\n",
    "    \"Hemoglobin\": r\"[Hh]a?emoglobin[^\\n]*?([0-9.]+)\\s*g/[dl1I]+\",\n",
    "    \"WBC\": r\"WBC\\s+Count.*?([0-9]+)\\s*/(?:cu\\.mm|cmm)\",\n",
    "    \"Lactate\": r\"(Lactate)\\s+([0-9.]+)\",\n",
    "    \"pH\": r\"\\bpH\\b\\s*([0-9.]+)\",\n",
    "    \"Age\": r\"Age\\s*:\\s*([0-9]{1,3})\",\n",
    "    \"Gender\": r\"(?:Sex|Gender)\\s*:\\s*([A-Za-z]+)\",\n",
    "    \"Cholestrol\": r\"(Cholesterol)\\s+([0-9.]+)\",\n",
    "    \"HbA1c\": r\"HbA1c\\s*.*?([0-9.]+)\",\n",
    "}\n",
    "\n",
    "UNIT_MAP = {\n",
    "    \"Glucose\": \"mg/dL\",\n",
    "    \"Insulin\": \"¬µIU/mL\",\n",
    "    \"Creatinine\": \"mg/dL\",\n",
    "    \"Urea (BUN)\": \"mg/dL\",\n",
    "    \"Sodium\": \"mmol/L\",\n",
    "    \"Potassium\": \"mmol/L\",\n",
    "    \"Hemoglobin\": \"g/dL\",\n",
    "    \"WBC\": \"/cmm\",\n",
    "    \"Lactate\": \"mmol/L\",\n",
    "    \"pH\": \"\",\n",
    "    \"Age\": \"years\",\n",
    "    \"Gender\": \"\",\n",
    "    \"Cholestrol\": \"mg/dL\",\n",
    "    \"HbA1c\": \"%\",\n",
    "}\n",
    "\n",
    "\n",
    "def ocr_pdf_bytes_to_text(pdf_bytes: bytes) -> str:\n",
    "    # Use pdfplumber on bytes\n",
    "    text_pages = []\n",
    "    try:\n",
    "        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text_pages.append(page_text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    full_text = \"\\n\".join(text_pages).strip()\n",
    "\n",
    "    # If direct text is too small, fallback to OCR\n",
    "    if len(full_text) < 200:\n",
    "        images = convert_from_path(io.BytesIO(pdf_bytes), dpi=300)\n",
    "        ocr_pages = []\n",
    "        for img in images:\n",
    "            page_text = pytesseract.image_to_string(img)\n",
    "            ocr_pages.append(page_text)\n",
    "        full_text = \"\\n\".join(ocr_pages)\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def extract_parameter(text: str, parameter: str) -> Dict[str, Any]:\n",
    "    pattern = PATTERN_MAP.get(parameter)\n",
    "    result = {\n",
    "        \"name\": parameter,\n",
    "        \"value\": None,\n",
    "        \"unit\": UNIT_MAP.get(parameter, \"\"),\n",
    "        \"raw_match\": \"\",\n",
    "    }\n",
    "    if not pattern:\n",
    "        return result\n",
    "\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "    if not match:\n",
    "        return result\n",
    "\n",
    "    value_str = match.groups()[-1]\n",
    "    result[\"value\"] = value_str.strip()\n",
    "    result[\"raw_match\"] = match.group(0).strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_all_parameters(text: str) -> Dict[str, Dict[str, Any]]:\n",
    "    data = {}\n",
    "    for param in PARAMETERS:\n",
    "        data[param] = extract_parameter(text, param)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. LSTM MODEL + CLINICAL JSON\n",
    "# =========================\n",
    "MODEL_FEATURES = [\n",
    "    \"Heart Rate\", \"MAP\", \"Respiratory Rate\", \"Temperature\",\n",
    "    \"Glucose\", \"Creatinine\", \"BUN\", \"Sodium\", \"Potassium\",\n",
    "    \"Hemoglobin\", \"WBC\", \"Lactate\",\n",
    "    \"Fluid Balance\", \"Vasopressors\", \"Sedatives\", \"Antibiotics\", \"Insulin\",\n",
    "]\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"Heart Rate\": 75,\n",
    "    \"MAP\": 90,\n",
    "    \"Respiratory Rate\": 16,\n",
    "    \"Temperature\": 98.4,\n",
    "    \"Lactate\": 1.0,\n",
    "    \"Fluid Balance\": 0,\n",
    "    \"Vasopressors\": 0,\n",
    "    \"Sedatives\": 0,\n",
    "    \"Antibiotics\": 0,\n",
    "    \"Insulin\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class SimpleAttention(Layer):\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SimpleAttention, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(\n",
    "            name=\"att_w1\",\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.W2 = self.add_weight(\n",
    "            name=\"att_w2\",\n",
    "            shape=(self.units, 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            name=\"att_b1\",\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "        )\n",
    "        super(SimpleAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        h = tf.nn.tanh(tf.matmul(x, self.W1) + self.b1)\n",
    "        e = tf.squeeze(tf.matmul(h, self.W2), -1)\n",
    "        alpha = tf.nn.softmax(e)\n",
    "        context = x * tf.expand_dims(alpha, -1)\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "def load_ai_resources():\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
    "    model = load_model(MODEL_PATH, custom_objects={\"SimpleAttention\": SimpleAttention})\n",
    "\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        X_ref = np.load(SCALER_PATH)\n",
    "        X_flat = X_ref.reshape(-1, X_ref.shape[2])\n",
    "        means = np.mean(X_flat, axis=0)\n",
    "        stds = np.std(X_flat, axis=0)\n",
    "        stds[stds == 0] = 1.0\n",
    "    else:\n",
    "        means = np.zeros(len(MODEL_FEATURES))\n",
    "        stds = np.ones(len(MODEL_FEATURES))\n",
    "    return model, means, stds\n",
    "\n",
    "\n",
    "MODEL, MEANS, STDS = load_ai_resources()\n",
    "\n",
    "\n",
    "def preprocess_patient_data(extracted_params: Dict[str, Any]):\n",
    "    # extracted_params is output of extract_all_parameters\n",
    "    def get_val_from_params(key: str):\n",
    "        item = extracted_params.get(key, {})\n",
    "        val = item.get(\"value\")\n",
    "        if val in [None, \"N/A\", \"Not Found\"]:\n",
    "            return None\n",
    "        try:\n",
    "            clean = str(val).replace(\"H\", \"\").replace(\"L\", \"\").strip()\n",
    "            return float(clean)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    vector = []\n",
    "    vitals_for_report = {}\n",
    "\n",
    "    for feature in MODEL_FEATURES:\n",
    "        json_key = feature\n",
    "        if feature == \"BUN\":\n",
    "            json_key = \"Urea (BUN)\"\n",
    "        if feature == \"Cholesterol\":\n",
    "            json_key = \"Cholestrol\"\n",
    "\n",
    "        val = get_val_from_params(json_key)\n",
    "        if val is None:\n",
    "            val = DEFAULTS.get(feature, 0)\n",
    "\n",
    "        if feature == \"WBC\" and val > 1000:\n",
    "            val = val / 1000.0\n",
    "\n",
    "        vector.append(val)\n",
    "        vitals_for_report[feature] = val\n",
    "\n",
    "    patient_matrix = np.tile(vector, (24, 1))\n",
    "    normalized_matrix = (patient_matrix - MEANS) / STDS\n",
    "    input_tensor = normalized_matrix.reshape(1, 24, len(MODEL_FEATURES))\n",
    "\n",
    "    return input_tensor, vitals_for_report\n",
    "\n",
    "\n",
    "def build_clinical_json(risk_score: float, vitals: Dict[str, float], raw_params: Dict[str, Any]):\n",
    "    llm_context = {\n",
    "        \"patient_metrics\": {\n",
    "            \"mortality_risk\": float(risk_score),\n",
    "            \"glucose\": float(vitals.get(\"Glucose\", 0.0)),\n",
    "            \"creatinine\": float(vitals.get(\"Creatinine\", 0.0)),\n",
    "        },\n",
    "        \"conditions\": [],\n",
    "        \"avoid\": [],\n",
    "        \"recommend\": [],\n",
    "        \"summary\": \"\",\n",
    "    }\n",
    "\n",
    "    if risk_score > 0.60:\n",
    "        llm_context[\"conditions\"].append(\"Critical Stability Risk\")\n",
    "        llm_context[\"summary\"] = \"Patient is at HIGH RISK. Immediate metabolic intervention required.\"\n",
    "    elif risk_score > 0.30:\n",
    "        llm_context[\"conditions\"].append(\"Moderate Clinical Risk\")\n",
    "        llm_context[\"summary\"] = \"Patient requires dietary management and monitoring.\"\n",
    "    else:\n",
    "        llm_context[\"summary\"] = \"Patient is stable. Routine maintenance diet recommended.\"\n",
    "\n",
    "    glucose = vitals.get(\"Glucose\", 0.0)\n",
    "    hba1c_val = raw_params.get(\"HbA1c\", {}).get(\"value\", \"N/A\")\n",
    "\n",
    "    is_diabetic = False\n",
    "    if glucose > 126:\n",
    "        is_diabetic = True\n",
    "    if hba1c_val != \"N/A\":\n",
    "        try:\n",
    "            val = float(str(hba1c_val).replace(\"H\", \"\").replace(\"L\", \"\"))\n",
    "            if val > 6.5:\n",
    "                is_diabetic = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if is_diabetic:\n",
    "        llm_context[\"conditions\"].append(\"Diabetes (Type 2 / Hyperglycemia)\")\n",
    "        llm_context[\"avoid\"].extend([\"Fruit juices\", \"White bread\", \"Processed sugars\", \"High-GI foods\"])\n",
    "        llm_context[\"recommend\"].extend([\"Complex carbohydrates\", \"High fiber foods (>30g/day)\", \"Leafy greens\"])\n",
    "\n",
    "    creat = vitals.get(\"Creatinine\", 0.0)\n",
    "    if creat > 1.2:\n",
    "        llm_context[\"conditions\"].append(\"Renal Stress / Kidney Strain\")\n",
    "        llm_context[\"avoid\"].extend([\"High sodium foods\", \"Excessive red meat\", \"Processed deli meats\"])\n",
    "        llm_context[\"recommend\"].extend([\"Low-potassium vegetables\", \"Cauliflower\", \"Berries\"])\n",
    "\n",
    "    if vitals.get(\"MAP\", 90) > 100:\n",
    "        llm_context[\"conditions\"].append(\"Hypertension Risk\")\n",
    "        llm_context[\"avoid\"].append(\"Salt/Sodium\")\n",
    "        llm_context[\"recommend\"].append(\"DASH diet principles\")\n",
    "\n",
    "    if not llm_context[\"conditions\"]:\n",
    "        llm_context[\"conditions\"].append(\"General Health Maintenance\")\n",
    "        llm_context[\"recommend\"].append(\"Balanced diet with lean proteins and vegetables\")\n",
    "\n",
    "    return llm_context\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. FOOD KB + GEMINI DIET PLAN\n",
    "# =========================\n",
    "def load_and_tag_data(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    def get_tags(row):\n",
    "        tags = []\n",
    "        if row[\"Carbohydrate (g)\"] < 30 and \"sugar\" not in str(row[\"ingredients\"]).lower():\n",
    "            tags.append(\"diabetic_friendly\")\n",
    "            tags.append(\"low_sugar\")\n",
    "        if row[\"Protein (g)\"] > 10:\n",
    "            tags.append(\"high_protein\")\n",
    "        if row[\"Total Fat (g)\"] < 8:\n",
    "            tags.append(\"low_fat\")\n",
    "        if 5 < row[\"Protein (g)\"] < 15:\n",
    "            tags.append(\"renal_safe\")\n",
    "        return tags\n",
    "\n",
    "    df[\"medical_tags\"] = df.apply(get_tags, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_smart_candidates(df, clinical_insights):\n",
    "    if df.empty:\n",
    "        return {}\n",
    "\n",
    "    conditions = \" \".join(clinical_insights.get(\"conditions\", [])).lower()\n",
    "    avoid = \" \".join(clinical_insights.get(\"avoid\", [])).lower()\n",
    "\n",
    "    candidates = df.copy()\n",
    "\n",
    "    if \"diabetes\" in conditions or \"sugar\" in avoid:\n",
    "        candidates = candidates[candidates[\"medical_tags\"].apply(lambda x: \"diabetic_friendly\" in x)]\n",
    "\n",
    "    if \"renal\" in conditions or \"kidney\" in conditions:\n",
    "        candidates = candidates[candidates[\"medical_tags\"].apply(lambda x: \"renal_safe\" in x)]\n",
    "\n",
    "    breakfast_keywords = \"idli|dosa|upma|poha|paratha|oats|porridge\"\n",
    "    breakfast_df = candidates[candidates[\"name\"].str.contains(breakfast_keywords, case=False, na=False)]\n",
    "    if len(breakfast_df) < 2:\n",
    "        breakfast_df = candidates.sample(n=min(5, len(candidates)))\n",
    "\n",
    "    mains_df = candidates[candidates[\"Energy (kcal)\"] > 150]\n",
    "    snacks_df = candidates[candidates[\"Energy (kcal)\"] < 150]\n",
    "\n",
    "    def serialize(sub_df, count=5):\n",
    "        return sub_df.sample(n=min(count, len(sub_df))).to_dict(orient=\"records\")\n",
    "\n",
    "    return {\n",
    "        \"breakfast\": serialize(breakfast_df),\n",
    "        \"lunch\": serialize(mains_df),\n",
    "        \"dinner\": serialize(mains_df),\n",
    "        \"snacks\": serialize(snacks_df),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_structured_plan(patient_profile, clinical_data, food_candidates):\n",
    "    summary = clinical_data.get(\"summary\", \"Healthy Diet\")\n",
    "    options_preview = json.dumps(food_candidates, indent=2)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI Clinical Dietitian.\n",
    "\n",
    "    PATIENT: {patient_profile['name']}\n",
    "    CONDITION: {summary}\n",
    "\n",
    "    TASK:\n",
    "    Select items from the PROVIDED CANDIDATE LIST below to create a 1-day meal plan.\n",
    "    You must output the result in STRICT JSON format matching the user's required schema.\n",
    "\n",
    "    CANDIDATE FOODS (Pick from these):\n",
    "    {options_preview}\n",
    "\n",
    "    REQUIRED OUTPUT FORMAT (JSON):\n",
    "    {{\n",
    "      \"day_plan\": {{\n",
    "          \"breakfast\": [\n",
    "              {{ \"item\": \"Name\", \"calories\": 100, \"protein\": 5, \"fat\": 2, \"carbs\": 10, \"tags\": [\"tag1\", \"tag2\"] }}\n",
    "          ],\n",
    "          \"lunch\": [],\n",
    "          \"dinner\": [],\n",
    "          \"snacks\": []\n",
    "      }},\n",
    "      \"total_nutrition\": {{\n",
    "          \"calories\": 0,\n",
    "          \"protein\": 0,\n",
    "          \"carbs\": 0,\n",
    "          \"fat\": 0\n",
    "      }},\n",
    "      \"medical_reasoning\": \"Brief explanation...\"\n",
    "    }}\n",
    "\n",
    "    RULES:\n",
    "    1. Use the EXACT nutritional values from the candidate list. Do not invent numbers.\n",
    "    2. Include the 'medical_tags' provided in the candidate list.\n",
    "    3. Calculate the 'total_nutrition' sum correctly.\n",
    "    4. Output JSON ONLY. No text before or after.\n",
    "    \"\"\"\n",
    "\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"day_plan\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"breakfast\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"item\", \"calories\", \"protein\", \"fat\", \"carbs\", \"tags\"],\n",
    "                        },\n",
    "                    },\n",
    "                    \"lunch\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"item\", \"calories\", \"protein\", \"fat\", \"carbs\", \"tags\"],\n",
    "                        },\n",
    "                    },\n",
    "                    \"dinner\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"item\", \"calories\", \"protein\", \"fat\", \"carbs\", \"tags\"],\n",
    "                        },\n",
    "                    },\n",
    "                    \"snacks\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"item\": {\"type\": \"string\"},\n",
    "                                \"calories\": {\"type\": \"number\"},\n",
    "                                \"protein\": {\"type\": \"number\"},\n",
    "                                \"fat\": {\"type\": \"number\"},\n",
    "                                \"carbs\": {\"type\": \"number\"},\n",
    "                                \"tags\": {\n",
    "                                    \"type\": \"array\",\n",
    "                                    \"items\": {\"type\": \"string\"},\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"item\", \"calories\", \"protein\", \"fat\", \"carbs\", \"tags\"],\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"breakfast\", \"lunch\", \"dinner\", \"snacks\"],\n",
    "            },\n",
    "            \"total_nutrition\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"calories\": {\"type\": \"number\"},\n",
    "                    \"protein\": {\"type\": \"number\"},\n",
    "                    \"fat\": {\"type\": \"number\"},\n",
    "                    \"carbs\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"calories\", \"protein\", \"fat\", \"carbs\"],\n",
    "            },\n",
    "            \"medical_reasoning\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"day_plan\", \"total_nutrition\", \"medical_reasoning\"],\n",
    "    }\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=GEMINI_MODEL_ID,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            temperature=0.1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if hasattr(resp, \"parsed\") and resp.parsed is not None:\n",
    "        return resp.parsed\n",
    "\n",
    "    return json.loads(resp.text.strip())\n",
    "\n",
    "\n",
    "def round_plan(plan: Dict[str, Any], ndigits: int = 0) -> Dict[str, Any]:\n",
    "    for meal in [\"breakfast\", \"lunch\", \"dinner\", \"snacks\"]:\n",
    "        for dish in plan.get(\"day_plan\", {}).get(meal, []):\n",
    "            for key in [\"calories\", \"protein\", \"fat\", \"carbs\"]:\n",
    "                val = dish.get(key)\n",
    "                if isinstance(val, (int, float)):\n",
    "                    dish[key] = round(val, ndigits)\n",
    "\n",
    "    tn = plan.get(\"total_nutrition\", {})\n",
    "    for key in [\"calories\", \"protein\", \"fat\", \"carbs\"]:\n",
    "        val = tn.get(key)\n",
    "        if isinstance(val, (int, float)):\n",
    "            tn[key] = round(val, ndigits)\n",
    "    return plan\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. FastAPI endpoint\n",
    "# =========================\n",
    "@app.post(\"/plan-diet\")\n",
    "async def plan_diet(report: UploadFile = File(...)):\n",
    "    if report.content_type not in [\"application/pdf\"]:\n",
    "        raise HTTPException(status_code=400, detail=\"Only PDF files are supported\")\n",
    "\n",
    "    pdf_bytes = await report.read()\n",
    "\n",
    "    # 1) OCR + parameter extraction\n",
    "    text = ocr_pdf_bytes_to_text(pdf_bytes)\n",
    "    extracted_params = extract_all_parameters(text)\n",
    "\n",
    "    # 2) LSTM risk prediction\n",
    "    input_tensor, vitals = preprocess_patient_data(extracted_params)\n",
    "    prediction = MODEL.predict(input_tensor, verbose=0)[0][0]\n",
    "    clinical_json = build_clinical_json(float(prediction), vitals, extracted_params)\n",
    "\n",
    "    # 3) Diet plan\n",
    "    food_df = load_and_tag_data(FOOD_KB_FILE)\n",
    "    candidates = get_smart_candidates(food_df, clinical_json)\n",
    "    patient = {\"name\": \"From PDF\", \"age\": extracted_params.get(\"Age\", {}).get(\"value\", None) or 45}\n",
    "    diet_plan = generate_structured_plan(patient, clinical_json, candidates)\n",
    "    diet_plan = round_plan(diet_plan, ndigits=0)\n",
    "\n",
    "    # Combine clinical + diet if you like\n",
    "    result = {\n",
    "        \"clinical\": clinical_json,\n",
    "        \"diet\": diet_plan,\n",
    "    }\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nutricare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
